var Tokeniser = require('cbor/tokeniser');
var consts = require('cbor/consts');

var tokens = consts.tokens,
    types = consts.types;

// Recursive token-parsing function; will evaluate tokens and build up complete object representations in order.
/*jshint maxcomplexity:20 */
function decode(token, tokeniser)  {
    switch (token.type) {
        case tokens.VALUE :
            return token.value;
        case tokens.MAP_START :
            var obj = {};

            while (true) {
                var field = tokeniser.nextToken();
               
                if (field === null) {
                    throw new Error('Unexpected EOF (reading: map key)');
                }

                if (field.type === tokens.MAP_END) {
                    break;
                }

                var value = tokeniser.nextToken();

                if (value === null || value.type === tokens.MAP_END) {
                    throw new Error('Unexpected EOF (reading: map value)');
                }

                obj[decode(field, tokeniser)] = decode(value, tokeniser);
            }
            
            return obj;
        case tokens.ARRAY_START :
            var arr = [];

            while (true) {
                var element = tokeniser.nextToken();

                if (element === null) {
                    throw new Error('Unexpected EOF (reading: array value)');
                }

                if (element.type === tokens.ARRAY_END) {
                    break;
                }

                arr.push(decode(element, tokeniser));
            }

            return arr;
        case tokens.STRING_START :
            var chunks = [];

            while (true) {
                var chunk = tokeniser.nextToken();

                if (chunk === null) {
                    throw new Error('Unexpected EOF (reading: indefinite-length string');
                }

                if (chunk.type === tokens.STRING_END) {
                    break;
                }

                if (chunk.header.type !== token.header.type) {
                    throw new Error('Unexpected chunk type (' + chunk.header.type + ') within string');
                }

                chunks.push(chunk.value);
            }

            var joined;

            if (token.header.type === types.BYTES) {
                joined = Buffer.concat(chunks);
            }

            if (token.header.type === types.STRING) {
                joined = chunks.join('');
            }

            return joined;
        default :
            throw new Error('Unexpected token: ' + JSON.stringify(token));
    }
}

/**
 * Provide a higher-level interface around Tokeniser, to enable the reading of 
 * complex values as represented in arrays/objects etc.
 */
module.exports = function Decoder(initial, offset, length) {
     var tokeniser = (Buffer.isBuffer(initial)) ? new Tokeniser(initial, offset, length) : initial;

     this.hasRemaining = tokeniser.hasRemaining;

     /**
      * Return the next value, parsed recursively.
      * <P>
      * If no tokens can be read, a read error will be thrown.
      */
     this.nextValue = function() {
         if (tokeniser.hasRemaining()) {
             return decode(tokeniser.nextToken(), tokeniser);
         } else {
             throw new Error('Token stream exhausted');
         }
     };
};

